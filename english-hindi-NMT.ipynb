{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import unicode_literals, print_function, division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from numpy.random import rand, shuffle\n",
    "import scipy\n",
    "import statsmodels\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import keras\n",
    "from io import open\n",
    "import unicodedata\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from pickle import dump, load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "INDIC_NLP_LIB_HOME=r\"/home/arushi/Neural-Machine-Translation/anoopkunchukuttan-indic_nlp_library-eccde81/src\"\n",
    "INDIC_NLP_RESOURCES=r\"/home/arushi/Neural-Machine-Translation/indic_nlp_resources-master\"\n",
    "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
    "from indicnlp import common\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "from indicnlp import loader\n",
    "loader.load()\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def to_pairs(english_text, hindi_text):\n",
    "    english_lines = english_text.strip().split('\\n')\n",
    "    hindi_lines = hindi_text.strip().split('\\n')\n",
    "    pairs = []\n",
    "    for i in range(len(hindi_lines)):\n",
    "        pairs.append([])\n",
    "        pairs[i].append(pre_process_english_sentence(english_lines[i]))\n",
    "        pairs[i].append(pre_process_hindi_sentence(hindi_lines[i]))\n",
    "    return pairs\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(u',','')\n",
    "    text = text.replace(u'\"','')\n",
    "    text = text.replace(u'\"','')\n",
    "    text = text.replace(u\"‘‘\",'')\n",
    "    text = text.replace(u\"’’\",'')\n",
    "    text = text.replace(u\"''\",'')\n",
    "    text = text.replace(u\"।\",'')\n",
    "    text=text.replace(u',','')\n",
    "    text=text.replace(u'\"','')\n",
    "    text=text.replace(u'(','')\n",
    "    text=text.replace(u')','')\n",
    "    text=text.replace(u'\"','')\n",
    "    text=text.replace(u':','')\n",
    "    text=text.replace(u\"'\",'')\n",
    "    text=text.replace(u\"‘‘\",'')\n",
    "    text=text.replace(u\"’’\",'')\n",
    "    text=text.replace(u\"''\",'')\n",
    "    text=text.replace(u\".\",'')\n",
    "    text=text.replace(u\"-\",'')\n",
    "    text=text.replace(u\"।\",'')\n",
    "    text=text.replace(u\"?\",'')\n",
    "    text=text.replace(u\"\\\\\",'')\n",
    "    text=text.replace(u\"_\",'')\n",
    "    text= re.sub(\"'\", '', text)\n",
    "    text=re.sub('[0-9+\\-*/.%]', '', text)\n",
    "    text=text.strip()\n",
    "    text=re.sub(' +', ' ',text)\n",
    "    exclude = set(string.punctuation)\n",
    "    text= ''.join(ch for ch in text if ch not in exclude)\n",
    "    return text\n",
    "\n",
    "def pre_process_english_sentence(line):\n",
    "    line = line.lower()\n",
    "    line = clean_text(line)\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    line = line.split()\n",
    "    line = [re_print.sub('', w) for w in line]\n",
    "    line = [word for word in line if word.isalpha()]\n",
    "    line = ' '.join(line)\n",
    "    return line\n",
    "\n",
    "def pre_process_hindi_sentence(line):\n",
    "    line=re.sub('[a-zA-Z]', '', line)\n",
    "    line = clean_text(line)\n",
    "    remove_nuktas = False\n",
    "    factory = IndicNormalizerFactory()\n",
    "    normalizer = factory.get_normalizer(\"hi\",remove_nuktas)\n",
    "    line = normalizer.normalize(line)\n",
    "    tokens = list()\n",
    "    for t in indic_tokenize.trivial_tokenize(line):\n",
    "        tokens.append(t)\n",
    "    line = tokens\n",
    "    line = [word for word in line if not re.search(r'\\d', word)]\n",
    "    line = ' '.join(line)\n",
    "    line = 'START_ '+ line + ' _END'\n",
    "    return (line)\n",
    "\n",
    "english_text = load_doc('English')\n",
    "hindi_text = load_doc('Hindi')\n",
    "pairs = to_pairs(english_text, hindi_text)\n",
    "df = pd.DataFrame(pairs)\n",
    "df.columns = [\"eng\", \"mar\"]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30492\n",
      "39566\n"
     ]
    }
   ],
   "source": [
    "all_eng_words=set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "# Vocabulary of French \n",
    "all_marathi_words=set()\n",
    "for mar in lines.mar:\n",
    "    for word in mar.split():\n",
    "        if word not in all_marathi_words:\n",
    "            all_marathi_words.add(word)\n",
    "\n",
    "#print (all_eng_words)\n",
    "print (len(all_eng_words))\n",
    "print (len(all_marathi_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in lines.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "print (max_length_src)\n",
    "\n",
    "# Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in lines.mar:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "print (max_length_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(pairs):\n",
    "    input_lang = Lang('eng')\n",
    "    output_lang = Lang('hin')\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    return input_lang, output_lang\n",
    "\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = load_doc('English')\n",
    "hindi_text = load_doc('Hindi')\n",
    "\n",
    "pairs = to_pairs(english_text, hindi_text)\n",
    "df = pd.DataFrame(pairs)\n",
    "df.columns = [\"eng\", \"mar\"]\n",
    "df\n",
    "#pairs = array(pairs)\n",
    "#input_lang, output_lang = prepareData(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines= pd.read_table('mar.t', names=['eng', 'mar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_clean_data(pairs, 'english-hindi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset size that to be taken\n",
    "n_sentences = len(pairs)\n",
    "dataset = pairs[:n_sentences, :]\n",
    "n_training_samples = (int)(n_sentences/2)\n",
    "train, test = dataset[:n_training_samples], dataset[n_training_samples:]\n",
    "#shuffle(train)\n",
    "#shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_clean_data(dataset, 'english-hindi-both.pkl')\n",
    "save_clean_data(train, 'english-hindi-train.pkl')\n",
    "save_clean_data(test, 'english-hindi-test.pkl')\n",
    "dataset = load_clean_sentences('english-hindi-both.pkl')\n",
    "train = load_clean_sentences('english-hindi-train.pkl')\n",
    "test = load_clean_sentences('english-hindi-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare hindi tokenizer\n",
    "hindi_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "hindi_vocab_size = len(hindi_tokenizer.word_index) + 1\n",
    "hindi_length = max_length(dataset[:, 1])\n",
    "print('Hindi Vocabulary Size: %d' % hindi_vocab_size)\n",
    "print('Hindi Max Length: %d' % (hindi_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_sequences(hindi_tokenizer, hindi_length, train[:, 1])\n",
    "trainY = encode_output(trainY, hindi_vocab_size)\n",
    "# prepare validation data\n",
    "trainX = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "trainY = encode_sequences(hindi_tokenizer, hindi_length, test[:, 1])\n",
    "trainY = encode_output(trainY, hindi_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
