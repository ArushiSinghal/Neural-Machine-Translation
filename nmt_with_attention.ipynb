{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/arushi/Neural-Machine-Translation/anoopkunchukuttan-indic_nlp_library-eccde81/src/indicnlp/script/indic_scripts.py:116: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  ALL_PHONETIC_VECTORS= ALL_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n",
      "/home/arushi/Neural-Machine-Translation/anoopkunchukuttan-indic_nlp_library-eccde81/src/indicnlp/script/indic_scripts.py:117: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  TAMIL_PHONETIC_VECTORS=TAMIL_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n",
      "/home/arushi/Neural-Machine-Translation/anoopkunchukuttan-indic_nlp_library-eccde81/src/indicnlp/script/english_script.py:113: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  ENGLISH_PHONETIC_VECTORS=ENGLISH_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import unicode_literals, print_function, division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, CuDNNLSTM, TimeDistributed, Flatten, Dropout, Bidirectional, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import array, argmax\n",
    "from numpy.random import rand, shuffle\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from keras.utils import multi_gpu_model, to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import scipy\n",
    "import statsmodels\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import keras\n",
    "from io import open\n",
    "import unicodedata\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from pickle import dump, load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "INDIC_NLP_LIB_HOME=r\"/home/arushi/Neural-Machine-Translation/anoopkunchukuttan-indic_nlp_library-eccde81/src\"\n",
    "INDIC_NLP_RESOURCES=r\"/home/arushi/Neural-Machine-Translation/indic_nlp_resources-master\"\n",
    "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
    "from indicnlp import common\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "from indicnlp import loader\n",
    "loader.load()\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def to_pairs(english_text, hindi_text):\n",
    "    english_lines = english_text.strip().split('\\n')\n",
    "    hindi_lines = hindi_text.strip().split('\\n')\n",
    "    pairs = []\n",
    "    for i in range(len(hindi_lines)):\n",
    "        pairs.append([])\n",
    "        pairs[i].append(pre_process_english_sentence(english_lines[i]))\n",
    "        pairs[i].append(pre_process_hindi_sentence(hindi_lines[i]))\n",
    "    return pairs\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(u',','')\n",
    "    text = text.replace(u'\"','')\n",
    "    text = text.replace(u'\"','')\n",
    "    text = text.replace(u\"‘‘\",'')\n",
    "    text = text.replace(u\"’’\",'')\n",
    "    text = text.replace(u\"''\",'')\n",
    "    text = text.replace(u\"।\",'')\n",
    "    text=text.replace(u',','')\n",
    "    text=text.replace(u'\"','')\n",
    "    text=text.replace(u'(','')\n",
    "    text=text.replace(u')','')\n",
    "    text=text.replace(u'\"','')\n",
    "    text=text.replace(u':','')\n",
    "    text=text.replace(u\"'\",'')\n",
    "    text=text.replace(u\"‘‘\",'')\n",
    "    text=text.replace(u\"’’\",'')\n",
    "    text=text.replace(u\"''\",'')\n",
    "    text=text.replace(u\".\",'')\n",
    "    text=text.replace(u\"-\",'')\n",
    "    text=text.replace(u\"।\",'')\n",
    "    text=text.replace(u\"?\",'')\n",
    "    text=text.replace(u\"\\\\\",'')\n",
    "    text=text.replace(u\"_\",'')\n",
    "    text=text.replace(\"'\", \"\")\n",
    "    text=text.replace('\"', \"\")\n",
    "    text= re.sub(\"'\", '', text)\n",
    "    text=re.sub('[0-9+\\-*/.%]', '', text)\n",
    "    text=text.strip()\n",
    "    text=re.sub(' +', ' ',text)\n",
    "    exclude = set(string.punctuation)\n",
    "    text= ''.join(ch for ch in text if ch not in exclude)\n",
    "    return text\n",
    "\n",
    "def pre_process_english_sentence(line):\n",
    "    line = line.lower()\n",
    "    line = clean_text(line)\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    line = line.split()\n",
    "    line = [re_print.sub('', w) for w in line]\n",
    "    line = [word for word in line if word.isalpha()]\n",
    "    line = ' '.join(line)\n",
    "    line = '<start> '+ line + ' <end>'\n",
    "    return line\n",
    "\n",
    "def pre_process_hindi_sentence(line):\n",
    "    line=re.sub('[a-zA-Z]', '', line)\n",
    "    line = clean_text(line)\n",
    "    remove_nuktas = False\n",
    "    factory = IndicNormalizerFactory()\n",
    "    normalizer = factory.get_normalizer(\"hi\",remove_nuktas)\n",
    "    line = normalizer.normalize(line)\n",
    "    tokens = list()\n",
    "    for t in indic_tokenize.trivial_tokenize(line):\n",
    "        tokens.append(t)\n",
    "    line = tokens\n",
    "    line = [word for word in line if not re.search(r'\\d', word)]\n",
    "    line = ' '.join(line)\n",
    "    line = '<start> '+ line + ' <end>'\n",
    "    return (line)\n",
    "\n",
    "english_text = load_doc('English_')\n",
    "hindi_text = load_doc('Hindi_')\n",
    "pairs = to_pairs(english_text, hindi_text)\n",
    "#df = pd.DataFrame(pairs)\n",
    "#df.columns = [\"eng\", \"mar\"]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Convert to dictionary\n",
    "clean_pairs = np.array(pairs)\n",
    "print (type(clean_pairs))\n",
    "print (type(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: clean_english-hindi.pkl\n"
     ]
    }
   ],
   "source": [
    "save_clean_data(clean_pairs, 'clean_english-hindi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<start> fresh breath and shining teeth enhance your personality <end>] => [<start> ताजा साँसें और चमचमाते दाँत आपके व्यक्तित्व को निखारते हैं <end>]\n",
      "[<start> your selfconfidence also increases with teeth <end>] => [<start> दाँतों से आपका आत्मविश्वास भी बढ़ता है <end>]\n",
      "[<start> bacteria stay between our gums and teeth <end>] => [<start> हमारे मसूढ़ों और दाँतों के बीच बैक्टीरिया मौजूद होते हैं <end>]\n",
      "[<start> they make teeth dirty and breath stinky <end>] => [<start> ये दाँतों को गंदा और साँसों को बदबूदार बना देते हैं <end>]\n",
      "[<start> you may keep your teeth clean and breath fresh by the help of some easy tips given here <end>] => [<start> यहाँ दिए कुछ आसान नुस्खों की मदद से आप अपने दाँतों को स्वच्छ और साँसों को ताजा रख सकते हैं <end>]\n",
      "[<start> clean your teeth properly <end>] => [<start> दाँतों को ठीक से साफ करें <end>]\n",
      "[<start> it takes two to three minutes to clean your teeth properly <end>] => [<start> दाँतों को ठीक से साफ करने में दो से तीन मिनट का समय लगता है <end>]\n",
      "[<start> but most of the people give less than one minute for this <end>] => [<start> लेकिन ज्यादातर लोग इसके लिए एक मिनट से भी कम समय देते हैं <end>]\n",
      "[<start> drink plenty of water <end>] => [<start> खूब पानी पीएँ <end>]\n",
      "[<start> bacteria attack fast if the mouth dries up <end>] => [<start> मुँह सूखने पर बैक्टीरिया हमला तेज कर देते हैं <end>]\n",
      "[<start> with this stink comes from breath <end>] => [<start> इससे साँसों से बदबू आने लगती है <end>]\n",
      "[<start> by drinking plenty of water not only the leftover pieces of food gets cleaned but saliva also gets formed <end>] => [<start> खूब पानी पीने से न केवल खाने के बचे खुचे टुकड़े साफ हो जाते हैं बल्कि लार भी बनती है <end>]\n",
      "[<start> saliva has important role in keeping the mouth clean <end>] => [<start> मुँह साफ रखने में लार की खास भूमिका होती है <end>]\n",
      "[<start> saliva destroys those bacteria which create stink in breath <end>] => [<start> लार उन बैक्टीरिया को नष्ट करती है जो साँसों में बदबू पैदा करते हैं <end>]\n",
      "[<start> chew the sugarfree chewing gum <end>] => [<start> चबाएँ शुगर रहित चुइंग गम <end>]\n",
      "[<start> saliva is formed by chewing the chewing gum <end>] => [<start> चुइंग गम चबाने से लार बनती है <end>]\n",
      "[<start> chewing gum helps in keeping the teeth clean <end>] => [<start> चुइंग गम से दाँतों को साफ रखने में मदद मिलती है <end>]\n",
      "[<start> sugared chewing gum is not supposed to be good for health <end>] => [<start> शुगर युक्त गम को सेहत के लिए अच्छा नहीं माना जाता <end>]\n",
      "[<start> that is why dentists do not suggest chewing sugared chewing gum <end>] => [<start> इसलिए डेंटिस्ट शुगर युक्त गम को खाने की सलाह नहीं देते <end>]\n",
      "[<start> get the teeth checkedup regularly <end>] => [<start> नियमित रूप से कराएँ दाँतों की जाँच <end>]\n",
      "[<start> get the teeth checkedup with the dentists regularly <end>] => [<start> दंतचिकित्सक से दाँतों की जाँच नियमित रूप से कराएँ <end>]\n",
      "[<start> they solve small problems of the teeth easily <end>] => [<start> वे दाँतों की छोटी मोटी समस्या को आसानी से सुलझा देते हैं <end>]\n",
      "[<start> clean the mouth after meal <end>] => [<start> खाने के बाद मुँह साफ करें <end>]\n",
      "[<start> do clean the mouth every time with water after eating <end>] => [<start> हर बार खाने के बाद पानी से मुँह जरूर साफ करें <end>]\n",
      "[<start> the leftover pieces of food is cleaned by it <end>] => [<start> इससे खाने के बचे हुए टुकड़े साफ हो जाते हैं <end>]\n",
      "[<start> clean the teeth with the mixture of lemon and salt <end>] => [<start> नींबू और नमक के मिश्रण से दाँत साफ करें <end>]\n",
      "[<start> taking a spoon of salt pour three to four drops of lemon juice in that <end>] => [<start> एक चम्मच में नमक लेकर उसमें तीन से चार बूंदें नींबू की डाल लें <end>]\n",
      "[<start> clean the teeth every week with this mixture <end>] => [<start> हर हफ्ते इस मिश्रण से दाँतों को साफ करें <end>]\n",
      "[<start> not only teeth starts shining with this <end>] => [<start> इससे न केवल दाँत चमचमाने लगते हैं <end>]\n",
      "[<start> but one also gets relief from stinking breath <end>] => [<start> बल्कि साँसों की दुर्गंध से भी छुटकारा मिलता है <end>]\n",
      "[<start> if there is health there is everything <end>] => [<start> सेहत है तो सब है <end>]\n",
      "[<start> it is not less than a challenge to keep oneself healthy in today s busy life <end>] => [<start> आज की व्यस्त जिंदगी में खुद को स्वस्थ रखना किसी चुनौती से कम नहीं <end>]\n",
      "[<start> but you can keep yourself healthy by paying attention to your food habits and by bringing a little change in your lifestyle <end>] => [<start> लेकिन खानपान पर ध्यान और जीवनशैली में थोड़ा बदलाव लाकर आप खुद को स्वस्थ रख सकते हैं <end>]\n",
      "[<start> eat less fatty food <end>] => [<start> कम वसायुक्त आहार का करें सेवन <end>]\n",
      "[<start> eat only that food in which there is less fat and more fiber <end>] => [<start> खाना वही खाएँ जिसमें कम वसा और फाइबर ज्यादा हो <end>]\n",
      "[<start> fruits and vegetables have a ratio like this <end>] => [<start> फलों और सब्जियों में ऐसा ही अनुपात रहता है <end>]\n",
      "[<start> we get helped in controlling our weight by taking less fatty and fibered food <end>] => [<start> कम वसा और फाइबर वाले खाने से वजन नियंत्रित करने में मदद मिलती है <end>]\n",
      "[<start> you keep yourself far from many diseases by eating less fatty and fibered food <end>] => [<start> कम वसा और फाइबर वाले खाने से आप कई बीमारियों से भी दूर रहते हैं <end>]\n",
      "[<start> take less salt and alcohol <end>] => [<start> नमक और शराब का सेवन कम करें <end>]\n",
      "[<start> take the least quantity of salt and alcohol to control blood pressure and cholesterol <end>] => [<start> ब्लड प्रेशर और कोलेस्ट्राल को नियंत्रित करने के लिए नमक और शराब का सेवन कम से कम मात्रा में करें <end>]\n",
      "[<start> stop smoking <end>] => [<start> बंद करें धूम्रपान <end>]\n",
      "[<start> there is a danger of several diseases including cancer with smoking <end>] => [<start> धूम्रपान करने से कैंसर समेत कई बीमारियों का खतरा होता है <end>]\n",
      "[<start> try your best to quit it <end>] => [<start> इसे छोड़ने की पूरी कोशिश करें <end>]\n",
      "[<start> exercise daily <end>] => [<start> रोज करें व्यायाम <end>]\n",
      "[<start> you must include exercise in your daily routine <end>] => [<start> अपनी रोज की दिनचर्या में व्यायाम को जरूर शामिल करें <end>]\n",
      "[<start> you can prevent heart diseases colon cancer blood pressure and diabetes like diseases by regular exercise <end>] => [<start> नियमित व्यायाम से आप हृदय रोग कोलन कैंसर ब्लड प्रेशर और डायबिटीज जैसी बीमारियों से बचाव कर सकते हैं <end>]\n",
      "[<start> keep doing light physical activities <end>] => [<start> करते रहें हल्की फुल्की शारीरिक गतिविधियाँ <end>]\n",
      "[<start> keep doing small physical activities like climbing stairs gardening small domestic works or dancing <end>] => [<start> सीढ़ी चढ़ना बागवानी घर के छोटे मोटे काम या डांस करने जैसी हल्की फुल्की शारीरिक गतिविधियाँ जरूर करते रहें <end>]\n",
      "[<start> these retain the flexibility in the body <end>] => [<start> इनसे शरीर में लचीलापन बना रहता है <end>]\n",
      "[<start> if fever comes continuously you must get it checked up <end>] => [<start> यदि लगातार बुखार आ रहा है तो उसकी जाँच अवश्य करायें <end>]\n",
      "[<start> malaria kalajar tuberculoses starts with fever <end>] => [<start> मलेरिया कालाजार यक्ष्मा की शुरुआत बुखार से ही होती है <end>]\n",
      "[<start> summer season has started <end>] => [<start> गर्मी का मौसम शुरू हो चुका है <end>]\n",
      "[<start> do not eat stale food and the things being sold at road sides <end>] => [<start> बासी भोजन व सड़क किनारे बिकने वाले सामान का सेवन नहीं करें <end>]\n",
      "[<start> do not feed children fast food kurkure icecream <end>] => [<start> बच्चों को फास्ट फूड कुरकुरे आइसक्रीम नहीं खिलायें <end>]\n",
      "[<start> in case of vomiting and loosemotion feed the o or mixture of salt and sugar <end>] => [<start> कैदस्त होने पर नमक और चीनी का घोल या ओआरएस पिलायें <end>]\n",
      "[<start> drink pure water as much as you can <end>] => [<start> साफ पानी अधिक से अधिक पीयें <end>]\n",
      "[<start> i suffer from fever continuously <end>] => [<start> लगातार बुखार से पीड़ित हो <end>]\n",
      "[<start> kalajar malaria or high fiver may have happened <end>] => [<start> कालाजार मलेरिया या फिर हाई फीवर हो सकता है <end>]\n",
      "[<start> my child is suffering from both diarrhoea and malaria <end>] => [<start> मेरा बच्चा दस्त और बुखार दोनों पीड़ित है <end>]\n",
      "[<start> first feed the mixture of o after this get the child checkedup with a paediatrician <end>] => [<start> पहले ओआरएस का घोल पिलायें इसके बाद बच्चे को किसी शिशु रोग विशेषज्ञ से शीघ्र दिखायें <end>]\n",
      "[<start> fever occurs at an interval of two to three months since two and half to three years <end>] => [<start> ढाई तीन वर्षो से हर दो तीन माह पर फीवर हो जाता है <end>]\n",
      "[<start> not only this white coat also forms on tongue <end>] => [<start> यही नहीं जीभ पर सफेद परत जम जाती है <end>]\n",
      "[<start> drink plenty of water and get urine culture done <end>] => [<start> पानी की मात्रा ज्यादा लें और यूरीन कल्चर जाँच करायें <end>]\n",
      "[<start> what are the primary symptoms of kalajar <end>] => [<start> कालाजार के प्रारंभिक लक्षण क्या हैं <end>]\n",
      "[<start> i have light fever for some days <end>] => [<start> कुछ दिनों से हल्का फीवर रहता है <end>]\n",
      "[<start> get the fever checked up and only after the other essential tests kalajar can be verified <end>] => [<start> बुखार की जाँच करें और अन्य जरूरी जाँच के पश्चात ही कालाजार प्रमाणित हो सकता है <end>]\n",
      "[<start> fever goes down after taking the tablet of metacin and again fever increases up to <end>] => [<start> मेटासिन की गोली देने से बुखार उतर जाता है और फिर तक बुखार चढ़ जाता है <end>]\n",
      "[<start> xray and tcdc are normal <end>] => [<start> एक्सरे टीसीडीसी नार्मल है <end>]\n",
      "[<start> measure the fever every four hours <end>] => [<start> चार चार घंटे पर फीवर को मापें <end>]\n",
      "[<start> if fever is more then give a pill of paracetamol and take suggestions from a medicine specialist doctor <end>] => [<start> अगर से ज्यादा फीवर हो तो पैरासीटामोल की गोली दें और किसी औषधि विशेषज्ञ चिकित्सक से परामर्श लें <end>]\n",
      "[<start> what is h <end>] => [<start> एचआईवी क्या है <end>]\n",
      "[<start> how does it happens <end>] => [<start> यह कैसे होता है <end>]\n",
      "[<start> state the solutions of prevention <end>] => [<start> बचाव के उपाय बताएँ <end>]\n",
      "[<start> this is a kind of virus <end>] => [<start> यह एक प्रकार का वायरस होता है <end>]\n",
      "[<start> its spread occurs mainly through unprotected sex relation infected needle infected blood and from mother to children <end>] => [<start> इसका फैलाव मुख्यत असुरक्षित यौन संबंध संक्रमित सूई संक्रमित रक्त व माँ से बच्चों में होता है <end>]\n",
      "[<start> the disease immune capacity of the body keeps on decreasing with this <end>] => [<start> इससे शरीर की रोग प्रतिरोधक क्षमता कम होती जाती है <end>]\n",
      "[<start> what is the identification of bone tb and what is its treatment <end>] => [<start> बोन टीवी की क्या पहचान है तथा इसका क्या इलाज है <end>]\n",
      "[<start> pain in the bones continuous fever whether it is low or increasing by the evening deformation of bones along with pain are the symptoms of tuberculosis <end>] => [<start> हड्डियों में दर्द लगातार बुखार भले ही यह कम क्यों न हो या बुखार शाम में बढ़ जाये दर्द के साथसाथ हड्डियों में विकृति आ जाये तो यह बोन टीबी का लक्षण है <end>]\n",
      "[<start> bones become weak and starts melting <end>] => [<start> इससे हड्डियाँ कमजोर हो जाती हैं और गलने लगती हैं <end>]\n",
      "[<start> paralysis may also attack in this situation <end>] => [<start> इस परिस्थिति में लकवा भी मार सकता है <end>]\n",
      "[<start> to test it do digital xray take medicine of tuberculosis for a year and consult the specialist doctor <end>] => [<start> इसकी जाँच के लिए डिजिटल एक्सरे एमआरआई करायें टीबी की दवा एक वर्ष तक लें एवं विशेषज्ञ चिकित्सक से परामर्श लें <end>]\n",
      "[<start> keep a shawl necessarily over the body and run the fan to the minimum in the night while sleeping <end>] => [<start> रात में सोते समय शरीर पर एक चादर जरूर रखे एवं पंखा कम से कम चलायें <end>]\n",
      "[<start> if you do not feel comfortable with it take antiallergic tablet for ten days <end>] => [<start> इससे अगर अपने आप सही महसूस नहीं करते हैं तो एन्टीएलर्जिक टेबलेट दस दिनों तक लें <end>]\n",
      "[<start> children are suffering from prickly heat what to do <end>] => [<start> बच्चों को घमौंरी हो रही है क्या करें <end>]\n",
      "[<start> what is the home treatment of diarrhoea <end>] => [<start> डायरिया का घरेलू उपचार क्या है <end>]\n",
      "[<start> put two spoons of sugar and a pinch of salt in a glass of water and boil it <end>] => [<start> एक गिलास पानी में दो चम्मच चीनी एवं एक चुटकी नमक डालकर उबालकर दें <end>]\n",
      "[<start> does kalajar occur because of sun <end>] => [<start> धूप के कारण क्या कालाजार रोग होता है <end>]\n",
      "[<start> does its influence increase in summer <end>] => [<start> क्या गर्मी में इसका प्रकोप बढ़ जाता है <end>]\n",
      "[<start> kalajar is caused by the sting of sandfly <end>] => [<start> कालाजार बालू मक्खी के काटने से होता है <end>]\n",
      "[<start> its treatment is available in all the hospitals <end>] => [<start> इसका उपचार सभी अस्पतालों में है <end>]\n",
      "[<start> there is a tablet also available for this now <end>] => [<start> इसके लिए अब टेबलेट भी उपलब्ध हैं <end>]\n",
      "[<start> what should be done when a child suffers from whooping cough <end>] => [<start> किसी बच्चे को कुकुर खाँसी हो जाये तो क्या करना चाहिये <end>]\n",
      "[<start> administer the dpt vaccine to the child <end>] => [<start> बच्चे को डीपीटी का टीका लगायें <end>]\n",
      "[<start> get consulted with a specialist doctor and get the mucus tested for cough <end>] => [<start> किसी विशेषज्ञ चिकित्सक से दिखायें तथा खाँसी के लिए बलगम की जाँच करायें <end>]\n",
      "[<start> how much aware are you about the care of your eyes <end>] => [<start> अपनी आँखों की देखभाल को लेकर आप कितने जागरूक हैं <end>]\n",
      "[<start> get eyes checked up every six months <end>] => [<start> हर छह माह में करें आँखों की जाँच <end>]\n",
      "[<start> maintain the glass or contact lens <end>] => [<start> चश्मे या कांटैक्ट लेंस का रखरखाव करें <end>]\n",
      "[<start> washing eyes regularly <end>] => [<start> नियमित आँखें धोना <end>]\n",
      "[<start> but along with all this if you pay attention to your foodhabits <end>] => [<start> लेकिन इन सबके साथ अगर आप अपने खान पान पर थोड़ा सा ध्यान दें <end>]\n",
      "[<start> not only the vision of your eyes may be better <end>] => [<start> तो न सिर्फ आपकी आँखों की रोशनी बेहतर हो सकती है <end>]\n"
     ]
    }
   ],
   "source": [
    "# spot check\n",
    "for i in range(100):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[%s] => [%s]' % (clean_pairs[24999,0], clean_pairs[24999,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('clean_english-hindi.pkl')\n",
    "\n",
    "n_sentences = len(clean_pairs)\n",
    "n_train = (int)(n_sentences/2)\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "\n",
    "train, test = dataset[:n_train], dataset[n_train:]\n",
    "\n",
    "shuffle(train)\n",
    "shuffle(test)\n",
    "shuffle(dataset)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "save_clean_data(dataset, 'clean_english-hindi-both.pkl')\n",
    "save_clean_data(train, 'clean_english-hindi-train.pkl')\n",
    "save_clean_data(test, 'clean_english-hindi-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('clean_english-hindi-both.pkl')\n",
    "train = load_clean_sentences('clean_english-hindi-train.pkl')\n",
    "test = load_clean_sentences('clean_english-hindi-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "  def __init__(self, lang):\n",
    "    self.lang = lang\n",
    "    self.word2idx = {}\n",
    "    self.idx2word = {}\n",
    "    self.vocab = set()\n",
    "    \n",
    "    self.create_index()\n",
    "    \n",
    "  def create_index(self):\n",
    "    for phrase in self.lang:\n",
    "      self.vocab.update(phrase.split(' '))\n",
    "    \n",
    "    self.vocab = sorted(self.vocab)\n",
    "    \n",
    "    self.word2idx['<pad>'] = 0\n",
    "    for index, word in enumerate(self.vocab):\n",
    "      self.word2idx[word] = index + 1\n",
    "    \n",
    "    for word, index in self.word2idx.items():\n",
    "      self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    # creating cleaned input, output pairs\n",
    "    pairs = load_clean_sentences(path)\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(en for en, hi in pairs)\n",
    "    targ_lang = LanguageIndex(hi for en, hi in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # English sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, hi in pairs]\n",
    "    \n",
    "    # Hindi sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in hi.split(' ')] for en, hi in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 5000\n",
    "path_to_file = 'clean_english-hindi-both.pkl'\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_tensor)\n",
    "print(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (target_tensor)\n",
    "print (target_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "#dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "  if tf.test.is_gpu_available():\n",
    "    return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "  else:\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = 1 - np.equal(real, 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        #print (batch)\n",
    "        #print (inp, targ)\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    #sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'keep doing light physical activities', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, hi in test:\n",
    "    translate(en, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
